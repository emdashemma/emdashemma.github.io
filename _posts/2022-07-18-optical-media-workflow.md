---
layout: post
title: "Optical media workflow: researching the workflow"
description: "Developing a workflow for imaging optical media"
tags: optical-media
---
## Optical media workflow: researching the workflow üíø

Now that I had learned the basics of [how CDs work](https://emdashemma.github.io/2022/06/23/optical-media-background.html), phase two was to situate that in the actual UW collections and use that knowledge to work up a plan for providing ongoing access to these information carriers.

<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_10.jpeg' width="500" align="left" style="padding: 15px"> CDs appear throughout the archival collections‚Äîwe're primarily looking at administrative records from the 90s and early 2000s, when a variety of optical media formats were very common means of file storage and transfer. At a higher level, there's an interesting archival or curatorial question here about whether the physical CD is (or should be) actually the accessioned object or not. If a donor wants to transfer files that constitute their "papers" to the library on a CD, using CDs as the digital equivalent of a box or bag, then the CD isn't actually the content, it's just a carrier, in which case the object for preservation would be the files themselves. But for the sake of this project, we were looking at materials that were accessioned a while ago and were taken to have some artifactual value in their current form.
<br><br>
<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_11.jpeg' width="500" align="left" style="padding: 15px">
Despite the rosy forecasts of the 90s, these discs are not shelf-stable. As [Duryee (2014)](https://journal.code4lib.org/articles/9581) put it, "The role of optical media in archives has shifted in the past decade from preservation medium to at-risk format.‚Äù The end-of-life forecast depends on the format in question, but basically, the organic dye layer in a recordable disc will naturally degrade over time, exacerbated to some degree by light, temperature, and humidity conditions; the greatest risk for commercially pressed discs, and a significant factor for all of them, is mechanical damage from handling and surface abrasion that eventually passes the threshold for error correction.
<br><br><br><br>
<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_13.jpeg' width="500" align="left" style="padding: 15px">
So we got me set up with the tools that the Department already had and I went to town. The heart of the operation was a Nimbie disc robot that you can load discs into the top of and batch-process. The robot is named Johann. I originally named it after Johan van der Knijff, who's the digital preservation researcher at the Royal Library of the Netherlands who developed the software tools that we're using to control the robot. But after a few weeks, I noticed that I had been spelling Johann the German way (with two Ns), not the Dutch way (with one), and figured I'd accidentally named it after J.S. Bach; so, meet **Johann Sebastian Batch**.

Human Johan's software for Robot Johann is called [Iromlab](https://github.com/KBNLresearch/iromlab). It replaces the native software of the robot with tools scripted in Python that control the drivers that physically load and unload discs, then it analyzes the format of each disc, and if it's an audio disc, rips it to WAV files, and if it contains data, then it creates a disk image, which is a complete copy (although not a ‚Äúbit-for-bit‚Äù copy) of the entirety of the disk in a standardized disk image file format called an ISO.

There's a conversation in the field about whether disk images are best practice‚Äîin some instances, for example, huge hard drives, you can argue that it's an irresponsible approach, which takes a lot of processing time and storage space to make an exact copy of a lot of empty space around the one or two things that are actually meaningful. But for CDs, which are very small, and where the item-level analysis would be very slow, I felt comfortable proceeding with the assumption that it was an appropriate goal to create one disk image for each physical disc‚Äîthat ISO file could then be described and packaged for preservation in the existing digital preservation workflow.

The bulk of the hours that I spent on this project were at a level of investigation that did not feel immediately productive or clarifying. Iromlab is not a commercial plug-and-play experience: it was designed specifically for the Netherlands Library use case, so, for example, the user interface assumes that you're querying their catalog to find disc titles. 
<br><br>
As an example of the kind of tool research that I was doing, I discovered that the New York Public Library had [forked the Iromlab code on GitHub](https://github.com/NYPL/iromlab) and built out a slightly different version, which seemed promising at first, but it took me like a day just to figure out how to download their version, because it wasn‚Äôt packaged with an installer or anything. And then it wouldn't launch because they had added some undocumented configuration elements and it took me like two more days of reading their code side-by-side with the original to find what was throwing this error. Eventually I got it to work, only to discover that this NYPL wasn‚Äôt actually better for us, just different, so I reverted to the original with nothing to show for that week ... except a better understanding of the architecture of the tool.

Pretty much all of April was spent doing laps in this pattern of installation, configuration, debugging, reconfiguration, and scrolling discussion forums from 2004 about Joliet file system extensions. It was often felt frustrating, or at least fruitless. In hindsight, I can see that I was slowly getting better at it, learning how to dig into these tools and the ways they worked and didn't work, how to work with error messages and when to go back to the documentation, when to step away from something. In the aggregate, I do think that these are generalizable and really useful skills for any sort of digital preservation work.

<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_17.jpeg' width="800" align="center" style="padding: 15px">

And aside from troubleshooting the individual tools, it was also really an exercise in creative imagination to keep in mind how they were fitting into the project as a whole. It felt at times like there were too many variables to solve for, because in my head the workflow looked something like this slide‚Äîall question marks. I wasn't sure exactly what the input looked like, in terms of the actual state and variety of discs that we would have to process. I wasn't sure what the process looked like, primarily around how to confirm that the tools were working and were accurately imaging the discs and then giving us accurate fail messages, and so what due diligence looked like as far as quality control. And then, I wasn't sure what the output looked like, because ultimately the goal was a disk image and metadata, along with some capacity for reporting back to Special Collections what we'd done. But what metadata did we need, how should it be structured, and how could we generate simple and useful description of the contents of the discs for a curator?

<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_19.jpeg' width="800" align="center" style="padding: 15px">
Very gradually‚Äîwith the help of a 28-page rambling Google doc of notes and questions, and with the invaluable dialectic of a very patient media preservation librarian‚Äîthe three separate categories of question marks came into focus. For the input, I started working with batches of test discs, some newly burned discs with just one or two files on them, so they would run through the process quickly, which was helpful for troubleshooting, and then some random old stuff from an office closet box of old transfers and access copies, which was a better simulation of the variety and wear that we would be getting on actual discs. 
<br><br>
<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_20.jpeg' width="500" align="left" style="padding: 15px">
Finally, we did some very rough-and-ready accelerated aging by baking a few CDs in the tape-baking oven; I carried some around loose in my bag for a week to scuff them up, and I even grilled one, for a real stress test. This image second from the rightis a disc that I flexed when it was hot out of the oven, and it split radially and then delaminated completely‚Äîboth of those discs are from a single CD. And the grilled disc actually worked perfectly‚Äîthe data was obliterated but it _did_ pass through the workflow successfully and get rejected without getting jammed or anything, so it was a successful test!
<br><br><br>
<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_21.jpeg' width="500" align="left" style="padding: 15px">
For the processing, I worked out the configuration issues and learned to interpret the metadata from Iromlab to a point of relative confidence in its imaging, and learned to separate the two main kinds of errors that it was throwing into stuff that we could note but ignore, and stuff that was critical that required individual reimaging with a different tool. This mostly came through doing a deep dive on each disc that had failed in an original imaging batch‚Äîso I could pull that out of the batch and investigate it with a whole range of possible tools, creating four or five different disk images with different programs, comparing those and their metadata. 
<br><br><br>
<img src='https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/dirks_slide_22.jpeg' width="500" align="left" style="padding: 15px"> 
And for output, I've arrived at a recommendation for the structure and contents of metadata that should be included with the disk images in the digital preservation workflow. This process was kind of arduous because I‚Äôm a bit of a maximalist when it comes to description and documentation, I don‚Äôt want to delete some metadata file that Iromlab made during processing that could potentially be useful. But I‚Äôm coming around to the less-is-more philosophy, and more specifically to the tool-neutral approach, which is that we should be creating consistently described objects regardless of how the details of the workflow might change in the future‚Äîif Johann retires, and we‚Äôre not using Iromlab, can we still meet the metadata specifications?
<br><br>
My deliverable for this project was a document called "[Preservation Workflow for Imaging Optical Media](https://raw.githubusercontent.com/emdashemma/emdashemma.github.io/main/uploads/Optical_Media_Workflow_2022-06-03.pdf)." It is organized with an introduction and overview of the tools, setup and configuration, the workflow proper, and then an appendix with more details and backstory that might be useful context for what's recommended in the workflow but isn't strictly in-scope to actual processing. I learned a huge amount in the process‚Äîpersistence in fiddling/debugging; navigating the tension between over- and under-description; generating clear (but not excessive) documentation (I may not have achieved that parenthetical!)‚Äîand I hope that the workflow, or some refined version of it, serves UW Preservation and its many shiny discs for many years to come. üíøüíøüíø
